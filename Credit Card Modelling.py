# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PB1NQeV_SZiNbrOWYUcEzC1EKdpYbHnW
"""

# Importing libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Reading the data
credit_risk= pd.read_csv("UCI_Credit_Card.csv")
credit_risk.head()

### Here I am doing to copy the original data in data frame called df.
df= credit_risk.copy()
df.head()

#Information of the data
# Lets see the information of data
df.info()

"""We are working with a data set containing 25 features for 30,000 clients. "default.payment.next.month" is a feature and is the target variable we are trying to predict."""

df.tail()

"""Explanatory Data Analysis"""

#checkking number of rows and columns
df.shape

#checking for missing values
df.isnull().sum()

"""We do not find any missing values in our data set."""

#Checking dublicates
df.duplicated(subset=None,keep='last')

#checking data types
df.dtypes

# As we seen Column ID has no meaning here so, we will remove it
df.drop(["ID"], axis=1, inplace= True) #axis=1 -- column removal and inplcae= True --means change in the original data

# Lets check the statistics of data
df.describe()

"""#Analysis of column "EDUCATION"
####EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)
"""

# Lets see the value counts in EDUCATION columns:
df.EDUCATION.value_counts()

# From the Data Description given, we know that in df.EDUCATION, 5 and 6 represents "unknown"
#Changing 0,5 and 6 to keep it under 1 category.

df['EDUCATION'].replace({0:1,1:1,2:2,3:3,4:4,5:1,6:1}, inplace=True)
df.EDUCATION.value_counts()

"""#Aalysis of column "MARRIAGE"
####Marital status (1=married, 2=single, 3=others)
"""

# lets see the values count in column marriage
df['MARRIAGE'].value_counts()

# Here I am going to map 0 with 1
df['MARRIAGE'].replace({0:1,1:1,2:2,3:3}, inplace=True)
df['MARRIAGE'].value_counts()

"""#Analysis of column "PAY_0 to PAY_6"
####PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months, 9=payment delay for nine months and above)
"""

# Lets see the value counts in column 'PAY_0'
df['PAY_0'].value_counts()

"""#Data Visualization

So, By looking at the target varibale we could say that, our data is pretty much imbalance. We would like to make it balanced before going to trained the model.
"""

plt.figure(figsize=(8, 6))

# Plot countplot for both categories
sns.countplot(data=df, x='default.payment.next.month', hue='default.payment.next.month')
plt.xticks([0, 1], ['Not Defaulted', 'Defaulted'])
plt.title('Target Distribution')
plt.xlabel('')
plt.legend(['Not Defaulted', 'Defaulted'])

plt.show()

# Lets count it
df['default.payment.next.month'].value_counts()

"""We need to make this imbalanced data into balanced data before train the model."""

plt.figure(figsize=(6,6))
sns.histplot(df['AGE'], kde=True, color='blue', edgecolor='black')
sns.kdeplot(df['AGE'], color='yellow')
plt.xlabel('Age')
plt.ylabel('Density')
plt.title('Age Distribution with Trendline')
plt.grid(True)
plt.show()

# Assuming 'default.payment.next.month' is the column indicating default status
plt.figure(figsize=(6,6))
sns.countplot(x='SEX', hue='default.payment.next.month', data=df, palette='coolwarm')
plt.xticks([0,1], labels=["Male", "Female"])
plt.xlabel('Gender')
plt.title("Gender Distribution with Default Status")
plt.legend(title='Default Payment Next Month', labels=['Not Default', 'Default'])
plt.show()

# With EDUCATION columns
# (1=graduate school, 2=university, 3=high school, 4=others)
plt.figure(figsize=(10,6))
sns.countplot(x='EDUCATION', hue='default.payment.next.month', data=df, palette='Set2')
plt.xticks(ticks=[0, 1, 2, 3], labels=["Graduate School", "University", "High School", "others"])
plt.xlabel('Education')
plt.title("Education Distribution with Default Status")
plt.legend(title='Default Payment Next Month', labels=['Not Default', 'Default'])
plt.show()

# With MARRIAGE columns
# (0=married, 1=single, 2=others)
plt.figure(figsize=(10,6))
sns.countplot(x='MARRIAGE', hue='default.payment.next.month', data=df, palette='Set2')
plt.xticks(ticks=[0, 1, 2], labels=["married", "single", "others"])
plt.xlabel('Marriage')
plt.title("Marriage Distribution with Default Status")
plt.legend(title='Default Payment Next Month', labels=['Not Default', 'Default'])
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(df['LIMIT_BAL'], kde=True, color='skyblue', edgecolor='black')
plt.xlabel('Credit Limit (LIMIT_BAL)')
plt.ylabel('Density')
plt.title('Distribution of Credit Limit')
plt.show()

plt.subplots(figsize=(20,10))
plt.subplot(231)
plt.scatter(x=df.PAY_AMT1, y=df.BILL_AMT1, c='tab:red', s=2)
plt.xlabel('PAY_AMT1')
plt.ylabel('BILL_AMT1')

plt.subplot(232)
plt.scatter(x=df.PAY_AMT2, y=df.BILL_AMT2, c='tab:orange', s=1)
plt.xlabel('PAY_AMT2')
plt.ylabel('BILL_AMT2')
plt.title('Payment structure vs Bill amount in the last 6 months', fontsize=15)

plt.subplot(233)
plt.scatter(x=df.PAY_AMT3, y=df.BILL_AMT3, c='tab:green', s=1)
plt.xlabel('PAY_AMT3')
plt.ylabel('BILL_AMT3')

plt.subplot(234)
plt.scatter(x=df.PAY_AMT4, y=df.BILL_AMT4, c='tab:brown', s=1)
plt.xlabel('PAY_AMT4')
plt.ylabel('BILL_AMT4')

plt.subplot(235)
plt.scatter(x=df.PAY_AMT5, y=df.BILL_AMT5, c='tab:blue', s=1)
plt.xlabel('PAY_AMT5')
plt.ylabel('BILL_AMT5')

plt.subplot(236)
plt.scatter(x=df.PAY_AMT6, y=df.BILL_AMT6, c ='tab:purple', s=1)
plt.xlabel('PAY_AMT6')
plt.ylabel('BILL_AMT6')

plt.show()

"""## Creating Independent features and dependent features

"""

# independent features
X= df.drop(["default.payment.next.month"], axis =1)
# dependent features
y = df["default.payment.next.month"]
X.head()

"""#Scalling the features
####So, scalling the independent features are very important so that our model is not biased toward the higher range of values. To make all features in same range
####Here we are using StandardScaler
"""

from sklearn.preprocessing import StandardScaler
scaler= StandardScaler()
X= scaler.fit_transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20,
                                                    random_state=42)

"""#Balancing the data

#### Balancing the Dataset: By repeating this process for multiple minority class instances, SMOTE increases the number of minority class examples, effectively balancing the dataset between the minority and majority classes.

####For example, suppose you have a dataset with 90% majority class instances and 10% minority class instances. After applying ***SMOTE(Synthetic Minority Over-sampling Technique)***, the number of minority class instances may increase, making the dataset more balanced, such as having 50% minority class instances and 50% majority class instances.
"""

from sklearn.utils import resample
from collections import Counter

# Resample the minority class using random oversampling
X_train_resampled, y_train_resampled = resample(X_train[y_train == 1],
                                                y_train[y_train == 1],
                                                replace=True,
                                                n_samples=X_train[y_train == 0].shape[0],
                                                random_state=42)

# Concatenate the resampled data with the majority class
X_train_resampled = np.concatenate((X_train[y_train == 0], X_train_resampled))
y_train_resampled = np.concatenate((y_train[y_train == 0], y_train_resampled))

# summarize class distribution
print("Before oversampling:", Counter(y_train))
print("After oversampling:", Counter(y_train_resampled))

"""#Building Model
###Logistic Regression
####Random Forest Classifier
####XGBoost Classifier
####Support vector machine classifier
####Logistic regression Model:
"""

from sklearn.linear_model import LogisticRegression
logit = LogisticRegression()
logit.fit(X_train, y_train)

#predicting the model
pred_logit = logit.predict(X_test)

"""##Evaluation Matrix"""

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score

print("The accuracy of logit model is:", accuracy_score(y_test, pred_logit))
print(classification_report(y_test, pred_logit))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix_custom(y_true, y_pred, classes, cmap=plt.cm.Blues):
    """
    Plot confusion matrix using matplotlib.
    """
    # Calculate confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Create a figure and an axis
    plt.figure(figsize=(8, 6))
    sns.set(font_scale=1.2)  # Adjust font size for better readability
    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False, square=True,
                xticklabels=classes, yticklabels=classes)

    # Add labels and title
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)

    # Show plot
    plt.tight_layout()
    plt.show()

# Usage
plot_confusion_matrix_custom(y_test, pred_logit, classes=['Class 0', 'Class 1'])

"""####TN (True Negative): 3242

True negatives are instances where the model correctly predicted the negative class (Class 0) when the true class was also negative. In this case, the model correctly classified 3242 instances as negative.
FP (False Positive): 1445

False positives are instances where the model incorrectly predicted the positive class (Class 1) when the true class was negative (Class 0). In this case, the model incorrectly classified 1445 instances as positive when they were actually negative.
FN (False Negative): 445

False negatives are instances where the model incorrectly predicted the negative class (Class 0) when the true class was positive (Class 1). In this case, the model incorrectly classified 445 instances as negative when they were actually positive.
TP (True Positive): 868

True positives are instances where the model correctly predicted the positive class (Class 1) when the true class was also positive. In this case, the model correctly classified 868 instances as positive.
Here's a breakdown of the interpretation:

Accuracy: It's the overall correctness of the model, calculated as the ratio of correctly classified instances (TP + TN) to the total number of instances.

Precision: It measures the model's ability to correctly identify positive instances, calculated as TP / (TP + FP). Higher precision indicates fewer false positives.

Recall (Sensitivity): It measures the model's ability to correctly identify all positive instances, calculated as TP / (TP + FN). Higher recall indicates fewer false negatives.

F1-score: It combines precision and recall into a single metric, calculated as the harmonic mean of precision and recall. It provides a balanced measure of the model's performance.

Specificity: It measures the model's ability to correctly identify all negative instances, calculated as TN / (TN + FP). Higher specificity indicates fewer false positives.
"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming logit is your trained logistic regression model,
# X_test is the independent features of the test dataset,
# y_test is the true labels of the test dataset
y_scores = logit.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

# Assuming logit is your trained logistic regression model,
# X_test is the independent features of the test dataset,
# y_test is the true labels of the test dataset
y_scores = logit.predict_proba(X_test)[:,1]

precision, recall, _ = precision_recall_curve(y_test, y_scores)

plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

"""##Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
# Fitting the model
rf.fit(X_train,y_train)

# Predicting the model
pred_rf= rf.predict(X_test)

"""##Evaluating the model"""

print("The accuracy of logit model is:", accuracy_score(y_test, pred_rf))
print(classification_report(y_test, pred_rf))

def plot_confusion_matrix_custom(y_true, y_pred, classes, cmap=plt.cm.Blues):
    """
    Plot confusion matrix using matplotlib.
    """
    # Calculate confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Create a figure and an axis
    plt.figure(figsize=(8, 6))
    sns.set(font_scale=1.2)  # Adjust font size for better readability
    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False, square=True,
                xticklabels=classes, yticklabels=classes)

    # Add labels and title
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)

    # Show plot
    plt.tight_layout()
    plt.show()

# Usage
plot_confusion_matrix_custom(y_test, pred_rf, classes=['Class 0', 'Class 1'])

# Assuming logit is your trained logistic regression model,
# X_test is the independent features of the test dataset,
# y_test is the true labels of the test dataset
y_scores = rf.predict_proba(X_test)[:,1]

precision, recall, _ = precision_recall_curve(y_test, y_scores)

plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

"""#####Precision: Precision measures the proportion of true positive predictions among all positive predictions made by the model. It represents the accuracy of the positive predictions. Precision is plotted on the y-axis.

Recall (Sensitivity): Recall, also known as sensitivity or true positive rate, measures the proportion of true positives that were correctly identified by the model out of all actual positives in the dataset. Recall is plotted on the x-axis.

Curve Shape: The Precision-Recall curve typically starts from the point (0,0) and moves towards the upper right corner of the plot. A curve closer to the upper right corner indicates better performance of the model, as it achieves high precision and high recall simultaneously.

Thresholds: Each point on the curve corresponds to a specific threshold used for classifying instances as positive or negative. The curve shows how precision and recall change as the threshold varies.

Interpretation: You can use the Precision-Recall curve to choose the appropriate threshold for your model based on your specific requirements. For example, if you prioritize precision over recall, you may want to select a threshold that maximizes precision while still maintaining an acceptable level of recall.

Overall, the Precision-Recall curve provides valuable insights into the performance of a binary classification model, especially when the classes are imbalanced or when the cost of false positives and false negatives varies.

## XGBoost Classifier
"""

import xgboost as xgb

xgb_clf= xgb.XGBClassifier()
#fitting the model
xgb_clf.fit(X_train,y_train)

## Predicting the model
xgb_predict= xgb_clf.predict(X_test)

"""##Evaluating the model"""

print("The accuracy of logit model is:", accuracy_score(y_test, xgb_predict))
print(classification_report(y_test,xgb_predict ))

def plot_confusion_matrix_custom(y_true, y_pred, classes, cmap=plt.cm.Blues):
    """
    Plot confusion matrix using matplotlib.
    """
    # Calculate confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Create a figure and an axis
    plt.figure(figsize=(8, 6))
    sns.set(font_scale=1.2)  # Adjust font size for better readability
    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False, square=True,
                xticklabels=classes, yticklabels=classes)

    # Add labels and title
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)

    # Show plot
    plt.tight_layout()
    plt.show()

# Usage
plot_confusion_matrix_custom(y_test, xgb_predict, classes=['Class 0', 'Class 1'])

import numpy as np
import xgboost as xgb
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

# Assuming xgb is your trained XGBoost model,
# X_test is the independent features of the test dataset,
# y_test is the true labels of the test dataset

# Train a new XGBoost model with a different objective that outputs raw scores
xgb_raw_scores = xgb.XGBClassifier(objective='reg:squarederror')
xgb_raw_scores.fit(X_train, y_train)

# Get predicted raw scores
y_scores_raw = xgb_raw_scores.predict(X_test)

# Apply sigmoid function to convert raw scores to probabilities
y_scores_proba = 1 / (1 + np.exp(-y_scores_raw))

precision, recall, _ = precision_recall_curve(y_test, y_scores_proba)

plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

"""#Hyperparameter tunning"""

## Hyper Parameter Optimization

params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]

}

## Hyperparameter optimization using RandomizedSearchCV
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

random_search=RandomizedSearchCV(xgb_clf,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)

# fitting the RandomizedSearchCV
random_search.fit(X_train,y_train)

# Finding the best estimators
random_search.best_estimator_

# Finding the best param
random_search.best_params_

import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Define the XGBClassifier and its parameters
classifier = xgb.XGBClassifier(objective='binary:logistic',
                                gamma=0.2,
                                learning_rate=0.15,
                                max_depth=15,
                                reg_lambda=10,
                                subsample=0.8,
                                colsample_bytree=0.8,
                                random_state=42)

# Train the classifier on the training data
classifier.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test data
y_pred = classifier.predict(X_test)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Predicting model
y_pred= classifier.predict(X_test)

"""#Evaluating the model after hyperparameter tuning"""

print("The accuracy of the model is:", accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

from sklearn.model_selection import cross_val_score
score=cross_val_score(classifier,X,y,cv=10)

score

score.mean()